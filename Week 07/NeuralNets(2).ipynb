{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks (NN)\n",
    "\n",
    "NNs are a hot topic not only in academia but also in big tech comanies, such as Google, Facebook, Amazon, etc.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning outcomes\n",
    "\n",
    "- real-world applications.\n",
    "- solving the wrong problem.\n",
    "- single- and multi-layer NN.\n",
    "- train a NN for handwritten image classification.\n",
    "- convolutional NN.\n",
    "- group activity (if time allows)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Real-world applications\n",
    "\n",
    "[i] Facebook's DeepFace for tagging images <br>\n",
    "[source](https://en.wikipedia.org/wiki/DeepFace); [paper](https://www.cs.toronto.edu/~ranzato/publications/taigman_cvpr14.pdf)\n",
    "- a deep learning facial recognition system.\n",
    "- identifies human faces in digital images.\n",
    "- uses a nine-layer NN with over 120 million connection weights.\n",
    "- was trained on 4 million images uploaded by Facebook users.\n",
    "- Facebook's DeepFace method accuracy: 97.35% (compared with FBI's Next Generation Identification system: 85%).\n",
    "\n",
    "[ii] Skin cancer classification <br>\n",
    "[source](https://cs.stanford.edu/people/esteva/nature/); [paper](https://www.nature.com/articles/nature21056)\n",
    "- a deep learning system trained to classify images of skin lessions as benign lesions or malignant skin cancers.\n",
    "- achives the accuracy of board-certified dermatologists.\n",
    "- pretain a deep neural network at general object recognition.\n",
    "- fine-tuned it on a dataset of ~130,000 skin lesion images comprised of over 2,000 diseases.\n",
    "\n",
    "`Other examples`\n",
    "\n",
    "[iii] Learning how to drive in dense traffic from camera video streams <br>\n",
    "[source](https://www.youtube.com/watch?v=X2s7gy3wIYw); [paper](https://arxiv.org/abs/1901.02705)\n",
    "\n",
    "[iv] Google's neural machine translation <br>\n",
    "[source](https://en.wikipedia.org/wiki/Google_Neural_Machine_Translation); [paper](https://arxiv.org/abs/1609.08144)\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Solving the wrong problem\n",
    "[source](https://physicsworld.com/a/neural-networks-explained/)\n",
    "\n",
    "[i] \"Roberto Novoa, a clinical dermatologist at Stanford University (re: part II, example (ii)), has described a time when he and his colleagues designed an algorithm to recognize skin cancer – only to discover that they’d accidentally designed a ruler detector instead, because the largest tumours had been photographed with rulers next to them for scale\". <br>\n",
    "\n",
    "[ii] \"Another group, this time at the University of Washington, demonstrated a deliberately bad algorithm that was, in theory, supposed to classify husky dogs and wolves, but actually functioned as a snow detector: they’d trained their algorithm with a dataset in which most of the wolf pictures had snowy backgrounds\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Basic concepts in NNs\n",
    "\n",
    "- single-layer NN: a single layer between the input and output nodes.\n",
    "- multi-layer NN: can add any number of hidden layers between between the input and output nodes.\n",
    "- artificial neurons represent the building blocks of the multi-layer NNs.\n",
    "- [a neat explanation of a basic multi-layer NN](https://www.youtube.com/watch?v=aircAruvnKk&t=1011s).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Classifying handwritten digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[paper](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf); [source](https://www.packtpub.com/data/python-machine-learning-third-edition)\n",
    "\n",
    "We will implement single-layer and multi-layer NNs to clasify handwritten digits from the popular Mixed National Institute of Standards and Technology (MNIST) dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1: Import packages\n",
    "\n",
    "We will introduce a new Python library, Theano, for working with NN. Theano is a popular choice for NN as the same code can be run on either CPUs or GPUs. GPUs greatly speed up the training and prediction, and is readily available. Amazon even offers GPU machines on EC2. \n",
    "\n",
    "If you'd like to go deeper into Theano, you may want to read this paper: http://www.iro.umontreal.ca/~lisa/pointeurs/theano_scipy2010.pdf\n",
    "\n",
    "Install Theano if you haven't already.  Then let's load it, and set it to work with a CPU.  For reference, here is the Theano documentation: http://www.deeplearning.net/software/theano/library/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.configdefaults): g++ not available, if using conda: `conda install m2w64-toolchain`\n",
      "C:\\Users\\cilin\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\theano\\configdefaults.py:560: UserWarning: DeprecationWarning: there is no c++ compiler.This is deprecated and with Theano 0.11 a c++ compiler will be mandatory\n",
      "  warnings.warn(\"DeprecationWarning: there is no c++ compiler.\"\n",
      "WARNING (theano.configdefaults): g++ not detected ! Theano will be unable to execute optimized C-implementations (for both CPU and GPU) and will default to Python implementations. Performance will be severely degraded. To remove this warning, set Theano flags cxx to an empty string.\n",
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import theano \n",
    "from theano import tensor as T\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "from theano.tensor.nnet.conv import conv2d\n",
    "from theano.tensor.signal.pool import max_pool_2d_same_size as max_pool_2d\n",
    "print (theano.config.device )# We're using CPUs (for now)\n",
    "print (theano.config.floatX )# Should be 64 bit for CPUs\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2: Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _onehot(y, n_classes):\n",
    "    \"\"\"Encode labels into one-hot representation\n",
    "\n",
    "    Parameters\n",
    "    ------------\n",
    "    y : array, shape = [n_examples]\n",
    "        Target values.\n",
    "\n",
    "    Returns\n",
    "    -----------\n",
    "    onehot : array, shape = (n_examples, n_labels)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    onehot = np.zeros((n_classes, y.shape[0]))\n",
    "    for idx, val in enumerate(y.astype(int)):\n",
    "        onehot[val, idx] = 1.\n",
    "    return onehot.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "your homework: comment the following functions following the example above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_min_max(dat):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    print(dat.min())\n",
    "    print(dat.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_1(X, w):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    return T.nnet.softmax(T.dot(X, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_2(X, w_1, w_2):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    return T.nnet.softmax(T.dot(T.nnet.sigmoid(T.dot(X, w_1)), w_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_3(X, w_1, w_2, w_3):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    return T.nnet.softmax(T.dot(T.nnet.sigmoid(T.dot(T.nnet.sigmoid(T.dot(X, w_1)), w_2)), w_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_rectifier(X, w_1, w_2):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    return T.nnet.softmax(T.dot(T.maximum(T.dot(X, w_1), 0.), w_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_dropout_1(X, w_1, w_2, p_1, p_2):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    return T.nnet.softmax(T.dot(dropout(T.maximum(T.dot(dropout(X, p_1), w_1),0.), p_2), w_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_dropout_2(X, w_1, w_2, w_3, p_1, p_2, p_3):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    return T.nnet.softmax(T.dot(dropout(T.maximum(T.dot(dropout(T.maximum(T.dot(dropout(X, p_1), w_1),0.), p_2), w_2),0.), p_3), w_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Theano provides built-in support for add convolutional layers\n",
    "def model_convonet(X, w_1, w_2, w_3, w_4, w_5, p_1, p_2):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    l1 = dropout(max_pool_2d(T.maximum(conv2d(X, w_1, border_mode='full'),0.), (2, 2)), p_1)\n",
    "    l2 = dropout(max_pool_2d(T.maximum(conv2d(l1, w_2), 0.), (2, 2)), p_1)\n",
    "    l3 = dropout(T.flatten(max_pool_2d(T.maximum(conv2d(l2, w_3), 0.), (2, 2)), outdim=2), p_1) # flatten to switch back to 1d layers\n",
    "    l4 = dropout(T.maximum(T.dot(l3, w_4), 0.), p_2)\n",
    "    return T.nnet.softmax(T.dot(l4, w_5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout(X, p=0.):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    if p > 0:\n",
    "        X *= srng.binomial(X.shape, p=1 - p)\n",
    "        X /= 1 - p\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescentBatch(epochs):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    trainTime = 0.0\n",
    "    predictTime = 0.0\n",
    "    for i in range(epochs):\n",
    "        start_time = time.time()\n",
    "        cost = train(X_train[0:len(X_train)], y_train_b[0:len(X_train)])\n",
    "        trainTime =  trainTime + (time.time() - start_time)\n",
    "        print ('%d) accuracy = %.4f' %(i+1, np.mean(np.argmax(y_test_b, axis=1) == predict(X_test))))\n",
    "    print ('train time = %.2f' %(trainTime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescentStochastic(epochs):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    trainTime = 0.0\n",
    "    predictTime = 0.0\n",
    "    start_time = time.time()\n",
    "    for i in range(epochs):       \n",
    "        for start, end in zip(range(0, len(X_train), miniBatchSize), range(miniBatchSize, len(X_train), miniBatchSize)):\n",
    "            cost = train(X_train[start:end], y_train_b[start:end])\n",
    "        trainTime =  trainTime + (time.time() - start_time)\n",
    "        print ('%d) accuracy = %.4f' %(i+1, np.mean(np.argmax(y_test_b, axis=1) == predict(X_test)))  )   \n",
    "    print ('train time = %.2f' %(trainTime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(cost, w):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    grads = T.grad(cost=cost, wrt=w)\n",
    "    updates = []\n",
    "    for w1, grad in zip(w, grads):\n",
    "        updates.append([w1, w1 - grad * alpha])\n",
    "    return updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop_convonet(cost, w, alpha=0.001, rho=0.9, epsilon=1e-6):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    grads = T.grad(cost=cost, wrt=w)\n",
    "    updates = []\n",
    "    for w1, grad in zip(w, grads):\n",
    "        \n",
    "        # adding gradient scaling\n",
    "        acc = theano.shared(w1.get_value() * 0.)\n",
    "        acc_new = rho * acc + (1 - rho) * grad ** 2\n",
    "        gradient_scaling = T.sqrt(acc_new + epsilon)\n",
    "        grad = grad / gradient_scaling\n",
    "        updates.append((acc, acc_new))\n",
    "        \n",
    "        updates.append((w1, w1 - grad * alpha))\n",
    "    return updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3: Read the MNIST dataset from the sklearn package\n",
    "\n",
    "The images in the MNIST dataset consist of 28 × 28 pixels. The first array returned by the fetch_openml function contains the 28 x 28 pixel images unrolled into 784 one-dimensional row vectors. The second array contains the the class labels (integers 0-9) of the handwritten digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784)\n",
      "(70000,)\n"
     ]
    }
   ],
   "source": [
    "X, y = fetch_openml(\"mnist_784\", version=1, return_X_y=True)\n",
    "\n",
    "y = y.astype(int)\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient-based optimization is much more stable under normalization, thus we want to normalize the pixel values in MNIST to the range -1 to 1 (originally 0 to 255)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixel range before normalization\n",
      "0.0\n",
      "255.0\n",
      "Pixel range after normalization\n",
      "-1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Pixel range before normalization\")\n",
    "print_min_max(X)\n",
    "\n",
    "print(\"Pixel range after normalization\")\n",
    "X = ((X / 255.0) - .5) * 2\n",
    "print_min_max(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create our training and test datasets. Let's cut the data for now, will make it easier to work with it. We will keep only 2,400 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 784)\n",
      "(400, 784)\n",
      "(2000,)\n",
      "(400,)\n"
     ]
    }
   ],
   "source": [
    "shuffle = np.random.permutation(np.arange(2400))\n",
    "X, y = X[shuffle], y[shuffle]\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(\n",
    "        X, y, test_size=400, \n",
    "        random_state=123, stratify=y)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now get an idea of how the images (digits 0-9) in the MNIST dataset look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=5, sharex=True, sharey=True)\n",
    "ax = ax.flatten()\n",
    "for i in range(10):\n",
    "    img = X_train[y_train == i][0].reshape(28, 28)\n",
    "    ax[i].imshow(img, cmap='Greys')\n",
    "\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also plot multiple examples of the same digit to see how different the handwritting for each really is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 25 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(nrows=5, ncols=5, sharex=True, sharey=True,)\n",
    "ax = ax.flatten()\n",
    "for i in range(25):\n",
    "    img = X_train[y_train == 7][i].reshape(28, 28)\n",
    "    ax[i].imshow(img, cmap='Greys')\n",
    "\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 4: Implementing a single-layer NN \n",
    "Let's prepare a variation of the label data.  Let's make these labels, rather than each being an integer value from 0-9, be a set of 10 binary values, one for each class.  This is sometimes called a 1-of-n encoding, and it makes working with NN easier, as there will be one output node for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2    3    4    5    6    7    8    9\n",
       "0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "1  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0\n",
       "2  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0\n",
       "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0\n",
       "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_classes = 10\n",
    "y_train_b = _onehot(y_train, n_classes)\n",
    "y_test_b = _onehot(y_test, n_classes)\n",
    "\n",
    "# temporary transformation to df for visualization purposes\n",
    "temp = pd.DataFrame(y_train_b)\n",
    "temp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets start with a KNN model to establish a baseline accuracy.\n",
    "\n",
    "Exercise: You've seen a number of different classification algorithms (e.g. naive bayes, decision trees, random forests, logistic regression) at this point.  How does KNN scalability and accuracy with respect to the size of the training dataset compare to those other algorithms?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train time = 0.05\n",
      "Accuracy = 0.9025\n",
      "Prediction time = 0.92\n"
     ]
    }
   ],
   "source": [
    "neighbors = 1\n",
    "\n",
    "knn = KNeighborsClassifier(neighbors)\n",
    "start_time = time.time()\n",
    "knn.fit(X_train, y_train)\n",
    "print ('Train time = %.2f' %(time.time() - start_time))\n",
    "\n",
    "start_time = time.time()\n",
    "accuracy = knn.score(X_test, y_test)\n",
    "print ('Accuracy = %.4f' %(accuracy))\n",
    "print ('Prediction time = %.2f' %(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, now that we have a simple baseline, let's start working in Theano. Before we jump to multi-layer neural networks though, let's train a **logistic regression** model to make certain we're using Theano correctly. <br>\n",
    "Recall from Josh's regression lecture the four key components: (1) parameters, (2) model, (3) cost function, and (4) objective. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.006061</td>\n",
       "      <td>0.020094</td>\n",
       "      <td>-0.000370</td>\n",
       "      <td>-0.008102</td>\n",
       "      <td>0.017610</td>\n",
       "      <td>0.012713</td>\n",
       "      <td>-0.016785</td>\n",
       "      <td>0.003765</td>\n",
       "      <td>0.010361</td>\n",
       "      <td>-0.004268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.000548</td>\n",
       "      <td>-0.003430</td>\n",
       "      <td>-0.006102</td>\n",
       "      <td>-0.000695</td>\n",
       "      <td>0.007819</td>\n",
       "      <td>0.005506</td>\n",
       "      <td>-0.001824</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>-0.027041</td>\n",
       "      <td>0.003832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001560</td>\n",
       "      <td>-0.003618</td>\n",
       "      <td>-0.003434</td>\n",
       "      <td>0.003011</td>\n",
       "      <td>-0.005941</td>\n",
       "      <td>0.000814</td>\n",
       "      <td>-0.008698</td>\n",
       "      <td>0.002247</td>\n",
       "      <td>0.012093</td>\n",
       "      <td>-0.001569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001172</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>0.002776</td>\n",
       "      <td>0.007061</td>\n",
       "      <td>-0.002175</td>\n",
       "      <td>-0.011460</td>\n",
       "      <td>0.003106</td>\n",
       "      <td>-0.005588</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>0.001883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.005020</td>\n",
       "      <td>0.011772</td>\n",
       "      <td>-0.015234</td>\n",
       "      <td>0.010951</td>\n",
       "      <td>-0.001272</td>\n",
       "      <td>0.008149</td>\n",
       "      <td>0.011437</td>\n",
       "      <td>0.013174</td>\n",
       "      <td>-0.007232</td>\n",
       "      <td>-0.010999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.006061  0.020094 -0.000370 -0.008102  0.017610  0.012713 -0.016785   \n",
       "1 -0.000548 -0.003430 -0.006102 -0.000695  0.007819  0.005506 -0.001824   \n",
       "2  0.001560 -0.003618 -0.003434  0.003011 -0.005941  0.000814 -0.008698   \n",
       "3  0.001172  0.000248  0.002776  0.007061 -0.002175 -0.011460  0.003106   \n",
       "4  0.005020  0.011772 -0.015234  0.010951 -0.001272  0.008149  0.011437   \n",
       "\n",
       "          7         8         9  \n",
       "0  0.003765  0.010361 -0.004268  \n",
       "1  0.001600 -0.027041  0.003832  \n",
       "2  0.002247  0.012093 -0.001569  \n",
       "3 -0.005588  0.000177  0.001883  \n",
       "4  0.013174 -0.007232 -0.010999  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## (1) Parameters \n",
    "# Initialize the weights to small, but non-zero, values.\n",
    "num_features = X_train[1].size\n",
    "num_classes = y_train_b[1].size\n",
    "w = theano.shared(np.asarray((np.random.randn(*(num_features, num_classes))*.01)))\n",
    "\n",
    "# temporary transformation to df for visualization purposes\n",
    "temp = pd.DataFrame(w.eval())\n",
    "print(temp.shape)\n",
    "temp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two notes relevant at this point: <br>\n",
    "\n",
    "First, logistic regression can be thought of as a neural network with no hidden layers. The output values are just the dot product of the inputs and the edge weights. <br>\n",
    "\n",
    "Second, we have 10 classes. We can either train separate one vs all classifiers using sigmoid activation, which would be a hassle, or we can use the **softmax activation**, which is essentially a multi-class version of sigmoid. We'll use Theano's built-in implementation of softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## (2) Model\n",
    "# Theano objects accessed with standard Python variables\n",
    "X = T.matrix()\n",
    "y = T.matrix()\n",
    "\n",
    "# label prediction\n",
    "y_hat = model_1(X, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use **cross-entropy** (a.k.a, log loss) as a cost function.  Cross-entropy only considers the error between the true class and the prediction, and not the errors for the false classes.  This tends to cause the network to converge faster.  We'll use Theano's built-in cross entropy function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## (3) Cost function\n",
    "cost = T.mean(T.nnet.categorical_crossentropy(y_hat, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective is minimize the cost, and to do that we'll use **batch gradient descent**.\n",
    "\n",
    "Exercise: What are the differences between batch, stochastic, and mini-batch gradient descent?  What are the implications of each for working on large datasets?\n",
    "\n",
    "We'll use Theano's built-in gradient function.  Exercise: Do you recall from Josh's lecture what the gradient is for beta in logistic regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) accuracy = 0.1825\n",
      "2) accuracy = 0.3225\n",
      "3) accuracy = 0.3925\n",
      "4) accuracy = 0.4300\n",
      "5) accuracy = 0.4650\n",
      "6) accuracy = 0.4975\n",
      "7) accuracy = 0.5400\n",
      "8) accuracy = 0.5775\n",
      "9) accuracy = 0.6050\n",
      "10) accuracy = 0.6225\n",
      "11) accuracy = 0.6425\n",
      "12) accuracy = 0.6625\n",
      "13) accuracy = 0.6650\n",
      "14) accuracy = 0.6825\n",
      "15) accuracy = 0.6850\n",
      "16) accuracy = 0.6975\n",
      "17) accuracy = 0.7025\n",
      "18) accuracy = 0.7150\n",
      "19) accuracy = 0.7200\n",
      "20) accuracy = 0.7225\n",
      "21) accuracy = 0.7250\n",
      "22) accuracy = 0.7300\n",
      "23) accuracy = 0.7350\n",
      "24) accuracy = 0.7350\n",
      "25) accuracy = 0.7375\n",
      "26) accuracy = 0.7450\n",
      "27) accuracy = 0.7500\n",
      "28) accuracy = 0.7500\n",
      "29) accuracy = 0.7525\n",
      "30) accuracy = 0.7550\n",
      "31) accuracy = 0.7550\n",
      "32) accuracy = 0.7575\n",
      "33) accuracy = 0.7625\n",
      "34) accuracy = 0.7650\n",
      "35) accuracy = 0.7700\n",
      "36) accuracy = 0.7725\n",
      "37) accuracy = 0.7725\n",
      "38) accuracy = 0.7750\n",
      "39) accuracy = 0.7775\n",
      "40) accuracy = 0.7800\n",
      "41) accuracy = 0.7825\n",
      "42) accuracy = 0.7850\n",
      "43) accuracy = 0.7875\n",
      "44) accuracy = 0.7875\n",
      "45) accuracy = 0.7875\n",
      "46) accuracy = 0.7900\n",
      "47) accuracy = 0.7900\n",
      "48) accuracy = 0.7925\n",
      "49) accuracy = 0.7925\n",
      "50) accuracy = 0.7975\n",
      "train time = 15.50\n",
      "predict time = 0.00\n"
     ]
    }
   ],
   "source": [
    "## (4) Objective (and solver)\n",
    "\n",
    "alpha = 0.01\n",
    "gradient = T.grad(cost=cost, wrt=w) \n",
    "update = [[w, w - gradient * alpha]] \n",
    "train = theano.function(inputs=[X, y], outputs=cost, updates=update, allow_input_downcast=True) # computes cost, then runs update\n",
    "y_pred = T.argmax(y_hat, axis=1) # select largest probability as prediction\n",
    "predict = theano.function(inputs=[X], outputs=y_pred, allow_input_downcast=True)\n",
    "\n",
    "# run batch gradient descent\n",
    "gradientDescentBatch(50)\n",
    "\n",
    "start_time = time.time()\n",
    "predict(X_test)   \n",
    "print ('predict time = %.2f' %(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise:  What do you expect to happen if we convert batch gradient descent to stochastic gradient descent?  Why?\n",
    "\n",
    "Let's try it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 10)\n",
      "1) accuracy = 0.7875\n",
      "2) accuracy = 0.8300\n",
      "3) accuracy = 0.8775\n",
      "4) accuracy = 0.8725\n",
      "5) accuracy = 0.8550\n",
      "6) accuracy = 0.8750\n",
      "7) accuracy = 0.8650\n",
      "8) accuracy = 0.8625\n",
      "9) accuracy = 0.8600\n",
      "10) accuracy = 0.8600\n",
      "11) accuracy = 0.8500\n",
      "12) accuracy = 0.8625\n",
      "13) accuracy = 0.8600\n",
      "14) accuracy = 0.8600\n",
      "15) accuracy = 0.8575\n",
      "16) accuracy = 0.8525\n",
      "17) accuracy = 0.8625\n",
      "18) accuracy = 0.8650\n",
      "19) accuracy = 0.8625\n",
      "20) accuracy = 0.8600\n",
      "21) accuracy = 0.8575\n",
      "22) accuracy = 0.8600\n",
      "23) accuracy = 0.8625\n",
      "24) accuracy = 0.8625\n",
      "25) accuracy = 0.8625\n",
      "26) accuracy = 0.8625\n",
      "27) accuracy = 0.8650\n",
      "28) accuracy = 0.8650\n",
      "29) accuracy = 0.8675\n",
      "30) accuracy = 0.8675\n",
      "31) accuracy = 0.8625\n",
      "32) accuracy = 0.8625\n",
      "33) accuracy = 0.8650\n",
      "34) accuracy = 0.8650\n",
      "35) accuracy = 0.8675\n",
      "36) accuracy = 0.8675\n",
      "37) accuracy = 0.8675\n",
      "38) accuracy = 0.8675\n",
      "39) accuracy = 0.8675\n",
      "40) accuracy = 0.8700\n",
      "41) accuracy = 0.8700\n",
      "42) accuracy = 0.8700\n",
      "43) accuracy = 0.8700\n",
      "44) accuracy = 0.8700\n",
      "45) accuracy = 0.8700\n",
      "46) accuracy = 0.8700\n",
      "47) accuracy = 0.8725\n",
      "48) accuracy = 0.8725\n",
      "49) accuracy = 0.8725\n",
      "50) accuracy = 0.8700\n",
      "train time = 1113.36\n",
      "predict time = 0.00\n"
     ]
    }
   ],
   "source": [
    "## (1) Parameters \n",
    "# Initialize the weights to small, but non-zero, values.\n",
    "num_features = X_train[1].size\n",
    "num_classes = y_train_b[1].size\n",
    "w = theano.shared(np.asarray((np.random.randn(*(num_features, num_classes))*.01)))\n",
    "\n",
    "# temporary transformation to df for visualization purposes\n",
    "temp = pd.DataFrame(w.eval())\n",
    "print(temp.shape)\n",
    "temp.head()\n",
    "\n",
    "\n",
    "## (2) Model\n",
    "# Theano objects accessed with standard Python variables\n",
    "X = T.matrix()\n",
    "y = T.matrix()\n",
    "\n",
    "# label prediction\n",
    "y_hat = model_1(X, w)\n",
    "\n",
    "\n",
    "## (3) Cost function\n",
    "cost = T.mean(T.nnet.categorical_crossentropy(y_hat, y))\n",
    "\n",
    "\n",
    "## (4) Objective (and solver)\n",
    "alpha = 0.01\n",
    "gradient = T.grad(cost=cost, wrt=w) \n",
    "update = [[w, w - gradient * alpha]] \n",
    "train = theano.function(inputs=[X, y], outputs=cost, updates=update, allow_input_downcast=True) # computes cost, then runs update\n",
    "y_pred = T.argmax(y_hat, axis=1) # select largest probability as prediction\n",
    "predict = theano.function(inputs=[X], outputs=y_pred, allow_input_downcast=True)\n",
    "\n",
    "# run stochastic gradient descent function\n",
    "miniBatchSize = 1 \n",
    "gradientDescentStochastic(50)\n",
    "\n",
    "start_time = time.time()\n",
    "predict(X_test)   \n",
    "print ('predict time = %.2f' %(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: What do you expect to happen if you switch the batch size to be great than 1 (i.e. mini-batch)?  Why?\n",
    "\n",
    "Try it for a few values..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 10)\n",
      "1) accuracy = 0.8150\n",
      "2) accuracy = 0.8475\n",
      "3) accuracy = 0.8600\n",
      "4) accuracy = 0.8600\n",
      "5) accuracy = 0.8625\n",
      "6) accuracy = 0.8625\n",
      "7) accuracy = 0.8625\n",
      "8) accuracy = 0.8700\n",
      "9) accuracy = 0.8750\n",
      "10) accuracy = 0.8750\n",
      "11) accuracy = 0.8750\n",
      "12) accuracy = 0.8700\n",
      "13) accuracy = 0.8675\n",
      "14) accuracy = 0.8675\n",
      "15) accuracy = 0.8675\n",
      "16) accuracy = 0.8650\n",
      "17) accuracy = 0.8675\n",
      "18) accuracy = 0.8700\n",
      "19) accuracy = 0.8750\n",
      "20) accuracy = 0.8775\n",
      "21) accuracy = 0.8775\n",
      "22) accuracy = 0.8800\n",
      "23) accuracy = 0.8800\n",
      "24) accuracy = 0.8800\n",
      "25) accuracy = 0.8825\n",
      "26) accuracy = 0.8825\n",
      "27) accuracy = 0.8825\n",
      "28) accuracy = 0.8825\n",
      "29) accuracy = 0.8825\n",
      "30) accuracy = 0.8825\n",
      "31) accuracy = 0.8825\n",
      "32) accuracy = 0.8800\n",
      "33) accuracy = 0.8800\n",
      "34) accuracy = 0.8800\n",
      "35) accuracy = 0.8800\n",
      "36) accuracy = 0.8800\n",
      "37) accuracy = 0.8800\n",
      "38) accuracy = 0.8800\n",
      "39) accuracy = 0.8775\n",
      "40) accuracy = 0.8800\n",
      "41) accuracy = 0.8800\n",
      "42) accuracy = 0.8800\n",
      "43) accuracy = 0.8800\n",
      "44) accuracy = 0.8800\n",
      "45) accuracy = 0.8800\n",
      "46) accuracy = 0.8800\n",
      "47) accuracy = 0.8800\n",
      "48) accuracy = 0.8800\n",
      "49) accuracy = 0.8800\n",
      "50) accuracy = 0.8800\n",
      "train time = 472.32\n",
      "predict time = 0.00\n"
     ]
    }
   ],
   "source": [
    "## (1) Parameters \n",
    "# Initialize the weights to small, but non-zero, values.\n",
    "num_features = X_train[1].size\n",
    "num_classes = y_train_b[1].size\n",
    "w = theano.shared(np.asarray((np.random.randn(*(num_features, num_classes))*.01)))\n",
    "\n",
    "# temporary transformation to df for visualization purposes\n",
    "temp = pd.DataFrame(w.eval())\n",
    "print(temp.shape)\n",
    "temp.head()\n",
    "\n",
    "## (2) Model\n",
    "# Theano objects accessed with standard Python variables\n",
    "X = T.matrix()\n",
    "y = T.matrix()\n",
    "\n",
    "# label prediction\n",
    "y_hat = model_1(X, w)\n",
    "\n",
    "## (3) Cost function\n",
    "cost = T.mean(T.nnet.categorical_crossentropy(y_hat, y))\n",
    "\n",
    "## (4) Objective (and solver)\n",
    "alpha = 0.01\n",
    "gradient = T.grad(cost=cost, wrt=w) \n",
    "update = [[w, w - gradient * alpha]] \n",
    "train = theano.function(inputs=[X, y], outputs=cost, updates=update, allow_input_downcast=True) # computes cost, then runs update\n",
    "y_pred = T.argmax(y_hat, axis=1) # select largest probability as prediction\n",
    "predict = theano.function(inputs=[X], outputs=y_pred, allow_input_downcast=True)\n",
    "\n",
    "# run stochastic gradient descent\n",
    "miniBatchSize = 10\n",
    "gradientDescentStochastic(50)\n",
    "\n",
    "start_time = time.time()\n",
    "predict(X_test)   \n",
    "print ('predict time = %.2f' %(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 4: Implementing a multi-layer NN\n",
    "\n",
    "We'll now implement a multi-layer perceptron to clasify the images in the MNIST dataset. Let's keep things simple for the moment, and implement a multi-layer NN with only one hidden layer, making it a two layer NN. <br>\n",
    "\n",
    "Because we have a hidden layer, we will now train the model using **backpropagation**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we start to get further into NN, if you'd like to take time on your own for an in-depth introduction on the state of the art in the topic, check out this excellent 1-day tutorial from KDD2014:\n",
    "\n",
    "Part 1: http://videolectures.net/kdd2014_bengio_deep_learning/\n",
    "\n",
    "Part 2: http://videolectures.net/tcmm2014_taylor_deep_learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) accuracy = 0.7100\n",
      "2) accuracy = 0.8275\n",
      "3) accuracy = 0.8575\n",
      "4) accuracy = 0.8600\n",
      "5) accuracy = 0.8650\n",
      "6) accuracy = 0.8675\n",
      "7) accuracy = 0.8700\n",
      "8) accuracy = 0.8775\n",
      "9) accuracy = 0.8725\n",
      "10) accuracy = 0.8750\n",
      "11) accuracy = 0.8775\n",
      "12) accuracy = 0.8750\n",
      "13) accuracy = 0.8800\n",
      "14) accuracy = 0.8825\n",
      "15) accuracy = 0.8825\n",
      "16) accuracy = 0.8850\n",
      "17) accuracy = 0.8825\n",
      "18) accuracy = 0.8825\n",
      "19) accuracy = 0.8825\n",
      "20) accuracy = 0.8825\n",
      "21) accuracy = 0.8825\n",
      "22) accuracy = 0.8850\n",
      "23) accuracy = 0.8850\n",
      "24) accuracy = 0.8850\n",
      "25) accuracy = 0.8850\n",
      "26) accuracy = 0.8850\n",
      "27) accuracy = 0.8825\n",
      "28) accuracy = 0.8825\n",
      "29) accuracy = 0.8825\n",
      "30) accuracy = 0.8850\n",
      "31) accuracy = 0.8825\n",
      "32) accuracy = 0.8775\n",
      "33) accuracy = 0.8775\n",
      "34) accuracy = 0.8800\n",
      "35) accuracy = 0.8825\n",
      "36) accuracy = 0.8825\n",
      "37) accuracy = 0.8825\n",
      "38) accuracy = 0.8850\n",
      "39) accuracy = 0.8850\n",
      "40) accuracy = 0.8875\n",
      "41) accuracy = 0.8875\n",
      "42) accuracy = 0.8900\n",
      "43) accuracy = 0.8900\n",
      "44) accuracy = 0.8900\n",
      "45) accuracy = 0.8900\n",
      "46) accuracy = 0.8900\n",
      "47) accuracy = 0.8900\n",
      "48) accuracy = 0.8875\n",
      "49) accuracy = 0.8875\n",
      "50) accuracy = 0.8875\n",
      "train time = 32976.15\n",
      "predict time = 0.42\n"
     ]
    }
   ],
   "source": [
    "## (1) Parameters\n",
    "num_hidden_nodes = 784\n",
    "w_1 = theano.shared(np.asarray((np.random.randn(*(num_features, num_hidden_nodes))*.01)))\n",
    "w_2 = theano.shared(np.asarray((np.random.randn(*(num_hidden_nodes, num_classes))*.01)))\n",
    "params = [w_1, w_2]\n",
    "\n",
    "\n",
    "## (2) Model\n",
    "X = T.matrix()\n",
    "Y = T.matrix()\n",
    "# Two notes:\n",
    "# First, feed forward is the composition of layers (dot product + activation function)\n",
    "# Second, activation on the hidden layer still uses sigmoid\n",
    "\n",
    "# label prediction\n",
    "y_hat = model_2(X, w_1, w_2)\n",
    "\n",
    "\n",
    "## (3) Cost...same as logistic regression\n",
    "cost = T.mean(T.nnet.categorical_crossentropy(y_hat, y))\n",
    "\n",
    "\n",
    "## (4) Minimization.  Update rule changes to backpropagation.\n",
    "alpha = 0.01\n",
    "update = backprop(cost, params)\n",
    "train = theano.function(inputs=[X, y], outputs=cost, updates=update, allow_input_downcast=True)\n",
    "y_pred = T.argmax(y_hat, axis=1)\n",
    "predict = theano.function(inputs=[X], outputs=y_pred, allow_input_downcast=True)\n",
    "\n",
    "# run stochastic gradient descent\n",
    "miniBatchSize = 1\n",
    "gradientDescentStochastic(50)\n",
    "\n",
    "start_time = time.time()\n",
    "predict(X_test)   \n",
    "print ('predict time = %.2f' %(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: Change the number of nodes in the hidden layer?  What do you expect the impact to be?  What is the impact?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) accuracy = 0.7050\n",
      "2) accuracy = 0.8200\n",
      "3) accuracy = 0.8550\n",
      "4) accuracy = 0.8575\n",
      "5) accuracy = 0.8650\n",
      "6) accuracy = 0.8675\n",
      "7) accuracy = 0.8675\n",
      "8) accuracy = 0.8675\n",
      "9) accuracy = 0.8700\n",
      "10) accuracy = 0.8700\n",
      "11) accuracy = 0.8700\n",
      "12) accuracy = 0.8750\n",
      "13) accuracy = 0.8775\n",
      "14) accuracy = 0.8775\n",
      "15) accuracy = 0.8825\n",
      "16) accuracy = 0.8875\n",
      "17) accuracy = 0.8825\n",
      "18) accuracy = 0.8800\n",
      "19) accuracy = 0.8800\n",
      "20) accuracy = 0.8800\n",
      "21) accuracy = 0.8775\n",
      "22) accuracy = 0.8800\n",
      "23) accuracy = 0.8800\n",
      "24) accuracy = 0.8825\n",
      "25) accuracy = 0.8850\n",
      "26) accuracy = 0.8850\n",
      "27) accuracy = 0.8850\n",
      "28) accuracy = 0.8875\n",
      "29) accuracy = 0.8875\n",
      "30) accuracy = 0.8875\n",
      "31) accuracy = 0.8900\n",
      "32) accuracy = 0.8900\n",
      "33) accuracy = 0.8925\n",
      "34) accuracy = 0.8925\n",
      "35) accuracy = 0.8925\n",
      "36) accuracy = 0.8900\n",
      "37) accuracy = 0.8900\n",
      "38) accuracy = 0.8900\n",
      "39) accuracy = 0.8900\n",
      "40) accuracy = 0.8900\n",
      "41) accuracy = 0.8900\n",
      "42) accuracy = 0.8900\n",
      "43) accuracy = 0.8925\n",
      "44) accuracy = 0.8925\n",
      "45) accuracy = 0.8925\n",
      "46) accuracy = 0.8925\n",
      "47) accuracy = 0.8925\n",
      "48) accuracy = 0.8925\n",
      "49) accuracy = 0.8900\n",
      "50) accuracy = 0.8875\n",
      "train time = 21710.74\n",
      "predict time = 0.29\n"
     ]
    }
   ],
   "source": [
    "## (1) Parameters\n",
    "num_hidden_nodes = 500\n",
    "w_1 = theano.shared(np.asarray((np.random.randn(*(num_features, num_hidden_nodes))*.01)))\n",
    "w_2 = theano.shared(np.asarray((np.random.randn(*(num_hidden_nodes, num_classes))*.01)))\n",
    "params = [w_1, w_2]\n",
    "\n",
    "\n",
    "## (2) Model\n",
    "X = T.matrix()\n",
    "Y = T.matrix()\n",
    "# Two notes:\n",
    "# First, feed forward is the composition of layers (dot product + activation function)\n",
    "# Second, activation on the hidden layer still uses sigmoid\n",
    "\n",
    "# label prediction\n",
    "y_hat = model_2(X, w_1, w_2)\n",
    "\n",
    "\n",
    "## (3) Cost...same as logistic regression\n",
    "cost = T.mean(T.nnet.categorical_crossentropy(y_hat, y))\n",
    "\n",
    "\n",
    "## (4) Minimization.  Update rule changes to backpropagation.\n",
    "alpha = 0.01\n",
    "update = backprop(cost, params)\n",
    "train = theano.function(inputs=[X, y], outputs=cost, updates=update, allow_input_downcast=True)\n",
    "y_pred = T.argmax(y_hat, axis=1)\n",
    "predict = theano.function(inputs=[X], outputs=y_pred, allow_input_downcast=True)\n",
    "\n",
    "# run stochastic gradient descent\n",
    "miniBatchSize = 1\n",
    "gradientDescentStochastic(50)\n",
    "\n",
    "start_time = time.time()\n",
    "predict(X_test)   \n",
    "print ('predict time = %.2f' %(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise:  We saw an improvement from adding a hidden layer.  What do you expect to happen if a second hidden layer was added?  \n",
    "\n",
    "Let's try it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) accuracy = 0.0950\n",
      "2) accuracy = 0.0950\n",
      "3) accuracy = 0.0950\n",
      "4) accuracy = 0.0950\n",
      "5) accuracy = 0.0950\n",
      "6) accuracy = 0.0950\n",
      "7) accuracy = 0.1600\n",
      "8) accuracy = 0.1100\n",
      "9) accuracy = 0.1100\n",
      "10) accuracy = 0.1475\n",
      "11) accuracy = 0.2200\n",
      "12) accuracy = 0.3325\n",
      "13) accuracy = 0.4000\n",
      "14) accuracy = 0.5025\n",
      "15) accuracy = 0.5925\n",
      "16) accuracy = 0.6525\n",
      "17) accuracy = 0.6975\n",
      "18) accuracy = 0.7300\n",
      "19) accuracy = 0.7725\n",
      "20) accuracy = 0.7875\n",
      "21) accuracy = 0.7825\n",
      "22) accuracy = 0.7950\n",
      "23) accuracy = 0.7900\n",
      "24) accuracy = 0.8000\n",
      "25) accuracy = 0.8125\n",
      "26) accuracy = 0.8225\n",
      "27) accuracy = 0.8425\n",
      "28) accuracy = 0.8475\n",
      "29) accuracy = 0.8500\n",
      "30) accuracy = 0.8600\n",
      "31) accuracy = 0.8650\n",
      "32) accuracy = 0.8725\n",
      "33) accuracy = 0.8675\n",
      "34) accuracy = 0.8675\n",
      "35) accuracy = 0.8650\n",
      "36) accuracy = 0.8675\n",
      "37) accuracy = 0.8700\n",
      "38) accuracy = 0.8625\n",
      "39) accuracy = 0.8650\n",
      "40) accuracy = 0.8650\n",
      "41) accuracy = 0.8675\n",
      "42) accuracy = 0.8750\n",
      "43) accuracy = 0.8825\n",
      "44) accuracy = 0.8800\n",
      "45) accuracy = 0.8825\n",
      "46) accuracy = 0.8825\n",
      "47) accuracy = 0.8825\n",
      "48) accuracy = 0.8825\n",
      "49) accuracy = 0.8800\n",
      "50) accuracy = 0.8800\n",
      "train time = 48363.83\n",
      "predict time = 0.65\n"
     ]
    }
   ],
   "source": [
    "## (1) Parms\n",
    "num_hidden_nodes = 600 \n",
    "w_1 = theano.shared(np.asarray((np.random.randn(*(num_features, num_hidden_nodes))*.01)))\n",
    "w_2 = theano.shared(np.asarray((np.random.randn(*(num_hidden_nodes, num_hidden_nodes))*.01)))\n",
    "w_3 = theano.shared(np.asarray((np.random.randn(*(num_hidden_nodes, num_classes))*.01)))\n",
    "params = [w_1, w_2, w_3]\n",
    "\n",
    "## (2) Model\n",
    "X = T.matrix()\n",
    "Y = T.matrix()\n",
    "\n",
    "# label prediction\n",
    "y_hat = model_3(X, w_1, w_2, w_3)\n",
    "\n",
    "\n",
    "## (3) Cost...same as logistic regression\n",
    "cost = T.mean(T.nnet.categorical_crossentropy(y_hat, y))\n",
    "\n",
    "\n",
    "## (4) Minimization.  Update rule changes to backpropagation.\n",
    "alpha = 0.01\n",
    "update = backprop(cost, params)\n",
    "train = theano.function(inputs=[X, y], outputs=cost, updates=update, allow_input_downcast=True)\n",
    "y_pred = T.argmax(y_hat, axis=1)\n",
    "predict = theano.function(inputs=[X], outputs=y_pred, allow_input_downcast=True)\n",
    "\n",
    "# run stochastic gradient descent\n",
    "miniBatchSize = 1 \n",
    "gradientDescentStochastic(50)\n",
    "\n",
    "start_time = time.time()\n",
    "predict(X_test)   \n",
    "print ('predict time = %.2f' %(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "As interest in networks with more layers and more complicated architechures has increased, a couple of tricks have emerged and become standard practice. Let's look at two of those--**rectifier activation** and **dropout noise**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activation Revisted\n",
    "\n",
    "Exercise: What is an activation function? What are common activation functions used in NN?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a recent idea around activation closely associated with deep learning.  In 2010, in a paper published at NIPS (https://www.utc.fr/~bordesan/dokuwiki/_media/en/glorot10nipsworkshop.pdf), Yoshua Bengio showed that *rectifier activation works better empirically than sigmoid activation when used in the hidden layers*.  \n",
    "\n",
    "The rectifier activation is simple: f(x)=max(0,x).  Intuitively, the difference is that as a sigmoid activated node approaches 1 it stops learning even if error continues to be propagated to it, whereas the rectifier activated node continue to learn (at least in the positive direction).  It is not completely understood (per Yoshua Bengio) why this helps, but there are some theories being explored including as related to the benefits of sparse representations in networks. (http://www.iro.umontreal.ca/~bengioy/talks/KDD2014-tutorial.pdf).  **Rectifiers also speed up training**.\n",
    "\n",
    "Although the paper was published in 2010, the technique didn't gain widespread adoption until 2012 when members of Hinton's group spread the word, including with this Kaggle entry: http://blog.kaggle.com/2012/11/01/deep-learning-how-i-did-it-merck-1st-place-interview/\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Let's change the activation in our 2 layer network to rectifier and see what happens..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) accuracy = 0.7850\n",
      "2) accuracy = 0.8175\n",
      "3) accuracy = 0.8725\n",
      "4) accuracy = 0.8600\n",
      "5) accuracy = 0.8625\n",
      "6) accuracy = 0.8900\n",
      "7) accuracy = 0.8850\n",
      "8) accuracy = 0.8675\n",
      "9) accuracy = 0.9100\n",
      "10) accuracy = 0.9100\n",
      "11) accuracy = 0.9125\n",
      "12) accuracy = 0.9150\n",
      "13) accuracy = 0.9175\n",
      "14) accuracy = 0.9175\n",
      "15) accuracy = 0.9200\n",
      "16) accuracy = 0.9175\n",
      "17) accuracy = 0.9175\n",
      "18) accuracy = 0.9175\n",
      "19) accuracy = 0.9175\n",
      "20) accuracy = 0.9175\n",
      "21) accuracy = 0.9175\n",
      "22) accuracy = 0.9125\n",
      "23) accuracy = 0.9125\n",
      "24) accuracy = 0.9125\n",
      "25) accuracy = 0.9125\n",
      "26) accuracy = 0.9125\n",
      "27) accuracy = 0.9125\n",
      "28) accuracy = 0.9125\n",
      "29) accuracy = 0.9125\n",
      "30) accuracy = 0.9125\n",
      "31) accuracy = 0.9125\n",
      "32) accuracy = 0.9125\n",
      "33) accuracy = 0.9125\n",
      "34) accuracy = 0.9125\n",
      "35) accuracy = 0.9125\n",
      "36) accuracy = 0.9125\n",
      "37) accuracy = 0.9125\n",
      "38) accuracy = 0.9125\n",
      "39) accuracy = 0.9125\n",
      "40) accuracy = 0.9125\n",
      "41) accuracy = 0.9125\n",
      "42) accuracy = 0.9125\n",
      "43) accuracy = 0.9125\n",
      "44) accuracy = 0.9125\n",
      "45) accuracy = 0.9125\n",
      "46) accuracy = 0.9125\n",
      "47) accuracy = 0.9125\n",
      "48) accuracy = 0.9125\n",
      "49) accuracy = 0.9125\n",
      "50) accuracy = 0.9125\n",
      "train time = 29974.04\n",
      "predict time = 0.01\n"
     ]
    }
   ],
   "source": [
    "## (1) Parms\n",
    "num_hidden_nodes = 784 \n",
    "w_1 = theano.shared(np.asarray((np.random.randn(*(num_features, num_hidden_nodes))*.01)))\n",
    "w_2 = theano.shared(np.asarray((np.random.randn(*(num_hidden_nodes, num_classes))*.01)))\n",
    "params = [w_1, w_2]\n",
    "\n",
    "\n",
    "## (2) Model\n",
    "X = T.matrix()\n",
    "Y = T.matrix()\n",
    "\n",
    "# label prediction\n",
    "y_hat = model_rectifier(X, w_1, w_2)\n",
    "\n",
    "\n",
    "## (3) Cost...same as logistic regression\n",
    "cost = T.mean(T.nnet.categorical_crossentropy(y_hat, y))\n",
    "\n",
    "\n",
    "## (4) Minimization.  Update rule changes to backpropagation.\n",
    "alpha = 0.01\n",
    "update = backprop(cost, params)\n",
    "train = theano.function(inputs=[X, y], outputs=cost, updates=update, allow_input_downcast=True)\n",
    "y_pred = T.argmax(y_hat, axis=1)\n",
    "predict = theano.function(inputs=[X], outputs=y_pred, allow_input_downcast=True)\n",
    "\n",
    "# run stochastic gradient descent\n",
    "miniBatchSize = 1 \n",
    "gradientDescentStochastic(50)\n",
    "\n",
    "start_time = time.time()\n",
    "predict(X_test)   \n",
    "print ('predict time = %.2f' %(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Maxout Activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So rectifier activation worked great!  \n",
    "\n",
    "Exercise: try another type of activation called Maxout (or Max Pooling) activiation.  Maxout activation just selects the max input as the output.  Maxout is a type of pooling, a technique which performs particularly well for vision problems. For more background see: http://jmlr.org/proceedings/papers/v28/goodfellow13.pdf and http://www.quora.com/What-is-impact-of-different-pooling-methods-in-convolutional-neural-networks  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropout noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is to simulate having a large number of different network architectures by **randomly dropping out nodes** during training. This is called dropout and offers a very computationally cheap and remarkably effective **regularization method* to reduce overfitting and improve generalization error in NN of all kinds.\n",
    "\n",
    "Hinton introduced the idea in 2012 and gave an explanation of why it's similar to bagging (http://arxiv.org/pdf/1207.0580v1.pdf)\n",
    "\n",
    "Let's give it a try..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) accuracy = 0.6900\n",
      "2) accuracy = 0.7175\n",
      "3) accuracy = 0.7575\n",
      "4) accuracy = 0.7750\n",
      "5) accuracy = 0.7350\n",
      "6) accuracy = 0.7875\n",
      "7) accuracy = 0.8300\n",
      "8) accuracy = 0.8550\n",
      "9) accuracy = 0.7325\n",
      "10) accuracy = 0.8525\n",
      "11) accuracy = 0.8675\n",
      "12) accuracy = 0.8500\n",
      "13) accuracy = 0.8600\n",
      "14) accuracy = 0.8675\n",
      "15) accuracy = 0.8225\n",
      "16) accuracy = 0.8850\n",
      "17) accuracy = 0.9050\n",
      "18) accuracy = 0.9025\n",
      "19) accuracy = 0.8325\n",
      "20) accuracy = 0.8775\n",
      "21) accuracy = 0.8850\n",
      "22) accuracy = 0.8800\n",
      "23) accuracy = 0.8950\n",
      "24) accuracy = 0.9075\n",
      "25) accuracy = 0.8700\n",
      "26) accuracy = 0.8375\n",
      "27) accuracy = 0.9025\n",
      "28) accuracy = 0.8950\n",
      "29) accuracy = 0.8975\n",
      "30) accuracy = 0.8450\n",
      "31) accuracy = 0.8925\n",
      "32) accuracy = 0.8550\n",
      "33) accuracy = 0.8700\n",
      "34) accuracy = 0.8975\n",
      "35) accuracy = 0.9200\n",
      "36) accuracy = 0.9100\n",
      "37) accuracy = 0.9000\n",
      "38) accuracy = 0.8800\n",
      "39) accuracy = 0.9050\n",
      "40) accuracy = 0.9000\n",
      "41) accuracy = 0.8850\n",
      "42) accuracy = 0.8925\n",
      "43) accuracy = 0.9000\n",
      "44) accuracy = 0.8950\n",
      "45) accuracy = 0.8975\n",
      "46) accuracy = 0.8425\n",
      "47) accuracy = 0.9100\n",
      "48) accuracy = 0.9000\n",
      "49) accuracy = 0.9000\n",
      "50) accuracy = 0.9000\n",
      "train time = 128176.60\n",
      "predict time = 0.00\n"
     ]
    }
   ],
   "source": [
    "## (1) Parms\n",
    "num_hidden_nodes = 784 \n",
    "w_1 = theano.shared(np.asarray((np.random.randn(*(num_features, num_hidden_nodes))*.01)))\n",
    "w_2 = theano.shared(np.asarray((np.random.randn(*(num_hidden_nodes, num_classes))*.01)))\n",
    "params = [w_1, w_2]\n",
    "\n",
    "\n",
    "## (2) Model\n",
    "X = T.matrix()\n",
    "Y = T.matrix()\n",
    "srng = RandomStreams()\n",
    "\n",
    "# predict label\n",
    "y_hat_train = model_dropout_1(X, w_1, w_2, 0.2, 0.5)\n",
    "y_hat_predict = model_dropout_1(X, w_1, w_2, 0., 0.)\n",
    "\n",
    "## (3) Cost...same as logistic regression\n",
    "cost = T.mean(T.nnet.categorical_crossentropy(y_hat_train, y))\n",
    "\n",
    "## (4) Minimization.  Update rule changes to backpropagation.\n",
    "alpha = 0.01\n",
    "update = backprop(cost, params)\n",
    "train = theano.function(inputs=[X, y], outputs=cost, updates=update, allow_input_downcast=True)\n",
    "y_pred = T.argmax(y_hat_predict, axis=1)\n",
    "predict = theano.function(inputs=[X], outputs=y_pred, allow_input_downcast=True)\n",
    "\n",
    "# run stochastic gradient descent\n",
    "miniBatchSize = 1\n",
    "gradientDescentStochastic(50)\n",
    "\n",
    "start_time = time.time()\n",
    "predict(X_test)   \n",
    "print ('predict time = %.2f' %(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try once again adding a second hidden layer..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) accuracy = 0.6625\n",
      "2) accuracy = 0.7525\n",
      "3) accuracy = 0.7800\n",
      "4) accuracy = 0.7825\n",
      "5) accuracy = 0.7825\n",
      "6) accuracy = 0.7225\n",
      "7) accuracy = 0.8325\n",
      "8) accuracy = 0.8300\n",
      "9) accuracy = 0.7650\n",
      "10) accuracy = 0.8800\n",
      "11) accuracy = 0.8800\n",
      "12) accuracy = 0.8125\n",
      "13) accuracy = 0.8075\n",
      "14) accuracy = 0.8950\n",
      "15) accuracy = 0.8450\n",
      "16) accuracy = 0.8775\n",
      "17) accuracy = 0.8575\n",
      "18) accuracy = 0.8200\n",
      "19) accuracy = 0.8850\n",
      "20) accuracy = 0.8600\n",
      "21) accuracy = 0.8875\n",
      "22) accuracy = 0.8525\n",
      "23) accuracy = 0.8725\n",
      "24) accuracy = 0.8850\n",
      "25) accuracy = 0.8375\n",
      "26) accuracy = 0.8875\n",
      "27) accuracy = 0.9000\n",
      "28) accuracy = 0.8875\n",
      "29) accuracy = 0.8300\n",
      "30) accuracy = 0.8350\n",
      "31) accuracy = 0.8875\n",
      "32) accuracy = 0.8575\n",
      "33) accuracy = 0.8525\n",
      "34) accuracy = 0.9025\n",
      "35) accuracy = 0.8775\n",
      "36) accuracy = 0.8450\n",
      "37) accuracy = 0.8775\n",
      "38) accuracy = 0.8500\n",
      "39) accuracy = 0.8600\n",
      "40) accuracy = 0.9050\n",
      "41) accuracy = 0.8800\n",
      "42) accuracy = 0.8525\n",
      "43) accuracy = 0.8375\n",
      "44) accuracy = 0.8600\n",
      "45) accuracy = 0.8375\n",
      "46) accuracy = 0.8800\n",
      "47) accuracy = 0.8900\n",
      "48) accuracy = 0.8700\n",
      "49) accuracy = 0.8750\n",
      "50) accuracy = 0.8700\n",
      "train time = 201090.72\n",
      "predict time = 0.05\n"
     ]
    }
   ],
   "source": [
    "## (1) Parmeters\n",
    "num_hidden_nodes = 784 \n",
    "w_1 = theano.shared(np.asarray((np.random.randn(*(num_features, num_hidden_nodes))*.01)))\n",
    "w_2 = theano.shared(np.asarray((np.random.randn(*(num_hidden_nodes, num_hidden_nodes))*.01)))\n",
    "w_3 = theano.shared(np.asarray((np.random.randn(*(num_hidden_nodes, num_classes))*.01)))\n",
    "params = [w_1, w_2, w_3]\n",
    "\n",
    "\n",
    "## (2) Model\n",
    "X = T.matrix()\n",
    "Y = T.matrix()\n",
    "\n",
    "srng = RandomStreams()\n",
    "\n",
    "# predict label\n",
    "y_hat_train = model_dropout_2(X, w_1, w_2, w_3, 0.2, 0.5,0.5)\n",
    "y_hat_predict = model_dropout_2(X, w_1, w_2, w_3, 0., 0.,0.)\n",
    "\n",
    "## (3) Cost...same as logistic regression\n",
    "cost = T.mean(T.nnet.categorical_crossentropy(y_hat_train, y))\n",
    "\n",
    "\n",
    "## (4) Minimization.  Update rule changes to backpropagation.\n",
    "alpha = 0.01\n",
    "update = backprop(cost, params)\n",
    "train = theano.function(inputs=[X, y], outputs=cost, updates=update, allow_input_downcast=True)\n",
    "y_pred = T.argmax(y_hat_predict, axis=1)\n",
    "predict = theano.function(inputs=[X], outputs=y_pred, allow_input_downcast=True)\n",
    "\n",
    "# run stochastic gradient descent\n",
    "miniBatchSize = 1\n",
    "gradientDescentStochastic(50)\n",
    "\n",
    "start_time = time.time()\n",
    "predict(X_train)   \n",
    "print ('predict time = %.2f' %(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5: Convolutional NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today, when the phrase 'deep learning' is used to describe a system, it is often a convolution NN (or convonet).  The convonet architechture was largely developed in the late 90's at Bell Labs, but only very recently popularized.  It was developed for image recognition, and is described and implemented with 2D representations in mind.\n",
    "\n",
    "Geoffrey Hinton has an excellent two-part lecture on the topic:\n",
    "\n",
    "https://www.youtube.com/watch?v=6oD3t6u5EPs\n",
    "\n",
    "https://www.youtube.com/watch?v=fueIAeAsGzA\n",
    "\n",
    "Also, this code was partly taken from these tutorials, which are worth referring back to:\n",
    "\n",
    "http://deeplearning.net/tutorial/lenet.html\n",
    "\n",
    "http://ufldl.stanford.edu/tutorial/supervised/FeatureExtractionUsingConvolution/\n",
    "\n",
    "https://www.youtube.com/watch?v=S75EdAcXHKk\n",
    "\n",
    "http://danielnouri.org/notes/2014/12/17/using-convolutional-neural-nets-to-detect-facial-keypoints-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## (1) Parameters\n",
    "num_hidden_nodes = 784 \n",
    "patch_width = 3\n",
    "patch_height = 3\n",
    "feature_maps_layer1 = 32\n",
    "feature_maps_layer2 = 64\n",
    "feature_maps_layer3 = 128\n",
    "\n",
    "# For convonets, we will work in 2d rather than 1d.  The MNIST images are 28x28 in 2d.\n",
    "image_width = 28\n",
    "X_train = X_train.reshape(-1, 1, image_width, image_width)\n",
    "X_test = X_test.reshape(-1, 1, image_width, image_width)\n",
    "\n",
    "# Convolution layers.  \n",
    "w_1 = theano.shared(np.asarray((np.random.randn(*(feature_maps_layer1, 1, patch_width, patch_height))*.01)))\n",
    "w_2 = theano.shared(np.asarray((np.random.randn(*(feature_maps_layer2, feature_maps_layer1, patch_width, patch_height))*.01)))\n",
    "w_3 = theano.shared(np.asarray((np.random.randn(*(feature_maps_layer3, feature_maps_layer2, patch_width, patch_height))*.01)))\n",
    "\n",
    "# Fully connected NN. \n",
    "w_4 = theano.shared(np.asarray((np.random.randn(*(feature_maps_layer3 * 3 * 3, num_hiddenNodes))*.01)))\n",
    "w_5 = theano.shared(np.asarray((np.random.randn(*(num_hidden_nodes, num_classes))*.01)))\n",
    "params = [w_1, w_2, w_3, w_4, w_5]\n",
    "\n",
    "## (2) Model\n",
    "X = T.tensor4() # conv2d works with tensor4 type\n",
    "Y = T.matrix()\n",
    "\n",
    "srng = RandomStreams()\n",
    "y_hat_train = model_convonet(X, w_1, w_2, w_3, w_4, w_5, 0.2, 0.5)\n",
    "y_hat_predict = model_convonet(X, w_1, w_2, w_3, w_4, w_5, 0., 0.)\n",
    "y_x = T.argmax(y_hat, axis=1)\n",
    "\n",
    "## (3) Cost\n",
    "cost = T.mean(T.nnet.categorical_crossentropy(y_hat_train, y))\n",
    "\n",
    "## (4) Minimization.  \n",
    "update = backprop_convonet(cost, params)\n",
    "train = theano.function(inputs=[X, y], outputs=cost, updates=update, allow_input_downcast=True)\n",
    "y_pred = T.argmax(y_hat_predict, axis=1)\n",
    "predict = theano.function(inputs=[X], outputs=y_pred, allow_input_downcast=True)\n",
    "\n",
    "miniBatchSize = 1\n",
    "gradientDescentStochastic(10)\n",
    "\n",
    "start_time = time.time()\n",
    "predict(test_data)   \n",
    "print ('predict time = %.2f' %(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional topic: Brain inspiration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The architechture of the convonet was inspired by the visual cortext in the human brain.  If you are interested in learning more, check out: http://www-psych.stanford.edu/~ashas/Cognition%20Textbook/chapter2.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional topic: Self-Driving Vehicles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional networks are starting to be used more and more for self-driving vehicles.  This past year, NVIDEA started produced a chipset for self-driving vehicles that analyzes the video of up to 14 cameras using convonets for \n",
    "\n",
    "CES 2015 parts 6,7,9\n",
    "\n",
    "https://www.youtube.com/watch?v=-vKGkxeflGw\n",
    "\n",
    "https://www.youtube.com/watch?v=zsVsUvx8ieo\n",
    "\n",
    "https://www.youtube.com/watch?v=RvQVyGOynFY\n",
    "\n",
    "GTC 2015 parts 4,5,7,9\n",
    "\n",
    "https://www.youtube.com/watch?v=pqvdZ2jp1NA\n",
    "\n",
    "https://www.youtube.com/watch?v=GGxdP_JWhwI\n",
    "\n",
    "https://www.youtube.com/watch?v=Tb7ZYSTYHbw\n",
    "\n",
    "https://www.youtube.com/watch?v=TDm6Snkle70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
